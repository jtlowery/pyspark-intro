{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rand, monotonically_increasing_id\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark - Debugging and Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Partitioning - In Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "cols = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "iris_df = pd.DataFrame(iris.data, columns=cols)\n",
    "iris_df[\"class\"] = iris.target\n",
    "iris_df[\"class\"] = iris_df[\"class\"].map({0: \"setosa\", 1: \"versicolor\", 2: \"virginica\"})\n",
    "iris_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width   class\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.createDataFrame(iris_df)\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .config(\"spark.default.parallelism\", 6)\n",
    "         .config(\"spark.default.parallelism\", 6)\n",
    "         .getOrCreate())\n",
    "df = spark.createDataFrame(iris_df)\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Checking config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.default.parallelism', '6'),\n",
       " ('spark.driver.host', '192.168.1.164'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.id', 'local-1613681321121'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.port', '34273'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.name', 'pyspark-shell')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Explicitly Repartitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(20).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(\"class\").rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(10, \"class\").rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 50, 0, 0, 50, 0, 0, 0, 50, 0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(10, \"class\").rdd.mapPartitions(lambda it: [sum(1 for _ in it)]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort(\"petal_length\").rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Input Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def save_parts(fp_format, df, num_parts):\n",
    "    parts = np.array_split(df, num_parts)\n",
    "    os.makedirs(os.path.dirname(fp_format), exist_ok=True)\n",
    "    for part_n, part in enumerate(parts):\n",
    "        part.to_csv(fp_format.format(part_n), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_parts(\"./3_parts/iris_part{}.csv\", iris_df, 3)\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .config(\"spark.default.parallelism\", 6)\n",
    "         .getOrCreate())\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"./3_parts/iris_part*.csv\")\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_parts(\"./20_parts/iris_part{}.csv\", iris_df, 20)\n",
    "spark = (SparkSession.builder\n",
    "         .config(\"spark.default.parallelism\", 6)\n",
    "         .getOrCreate())\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"./20_parts/iris_part*.csv\")\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Partitioning - On Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Partitions represented in directory structure with {partition_name}={value} format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df.write.parquet(\"./not_partitioned_df/\", mode=\"overwrite\", compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./not_partitioned_df/\u001b[00m\n",
      "├── part-00000-ac7d9d1b-176a-4ac8-a62a-12cc29f0f9c1-c000.snappy.parquet\n",
      "├── part-00001-ac7d9d1b-176a-4ac8-a62a-12cc29f0f9c1-c000.snappy.parquet\n",
      "├── part-00002-ac7d9d1b-176a-4ac8-a62a-12cc29f0f9c1-c000.snappy.parquet\n",
      "├── part-00003-ac7d9d1b-176a-4ac8-a62a-12cc29f0f9c1-c000.snappy.parquet\n",
      "├── part-00004-ac7d9d1b-176a-4ac8-a62a-12cc29f0f9c1-c000.snappy.parquet\n",
      "└── _SUCCESS\n",
      "\n",
      "0 directories, 6 files\n"
     ]
    }
   ],
   "source": [
    "!tree ./not_partitioned_df/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df.write.partitionBy(\"class\").parquet(\"./partitioned_df/\", mode=\"overwrite\", compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./partitioned_df/\u001b[00m\n",
      "├── \u001b[01;34mclass=setosa\u001b[00m\n",
      "│   ├── part-00000-3a42591a-a849-4b20-af29-ca91397873f1.c000.snappy.parquet\n",
      "│   ├── part-00001-3a42591a-a849-4b20-af29-ca91397873f1.c000.snappy.parquet\n",
      "│   ├── part-00002-3a42591a-a849-4b20-af29-ca91397873f1.c000.snappy.parquet\n",
      "│   └── part-00003-3a42591a-a849-4b20-af29-ca91397873f1.c000.snappy.parquet\n",
      "├── \u001b[01;34mclass=versicolor\u001b[00m\n",
      "│   ├── part-00000-3a42591a-a849-4b20-af29-ca91397873f1.c000.snappy.parquet\n",
      "│   └── part-00001-3a42591a-a849-4b20-af29-ca91397873f1.c000.snappy.parquet\n",
      "├── \u001b[01;34mclass=virginica\u001b[00m\n",
      "│   ├── part-00001-3a42591a-a849-4b20-af29-ca91397873f1.c000.snappy.parquet\n",
      "│   ├── part-00003-3a42591a-a849-4b20-af29-ca91397873f1.c000.snappy.parquet\n",
      "│   └── part-00004-3a42591a-a849-4b20-af29-ca91397873f1.c000.snappy.parquet\n",
      "└── _SUCCESS\n",
      "\n",
      "3 directories, 10 files\n"
     ]
    }
   ],
   "source": [
    "!tree ./partitioned_df/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Why bother with these partitions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [sepal_length#110, sepal_width#111, petal_length#112, petal_width#113, class#114]\n",
      "+- *(1) Filter (isnotnull(class#114) && (class#114 = setosa))\n",
      "   +- *(1) FileScan parquet [sepal_length#110,sepal_width#111,petal_length#112,petal_width#113,class#114] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/joel/repos/pyspark-pres/not_partitioned_df], PartitionFilters: [], PushedFilters: [IsNotNull(class), EqualTo(class,setosa)], ReadSchema: struct<sepal_length:string,sepal_width:string,petal_length:string,petal_width:string,class:string>\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"./not_partitioned_df\").filter(\"class = 'setosa'\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) FileScan parquet [sepal_length#120,sepal_width#121,petal_length#122,petal_width#123,class#124] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/joel/repos/pyspark-pres/partitioned_df], PartitionCount: 1, PartitionFilters: [isnotnull(class#124), (class#124 = setosa)], PushedFilters: [], ReadSchema: struct<sepal_length:string,sepal_width:string,petal_length:string,petal_width:string>\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"./partitioned_df\").filter(\"class = 'setosa'\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Why in one case does the output have 9 files instead of 5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Partitions in Memory -> Files on Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Highest number of files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .config(\"spark.default.parallelism\", 6)\n",
    "         .config(\"spark.sql.shuffle.partitions\", 5)\n",
    "         .getOrCreate())\n",
    "df = spark.createDataFrame(iris_df)\n",
    "shuffled_df = df.sort(rand(seed=1))\n",
    "shuffled_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "shuffled_df.write.partitionBy(\"class\").parquet(\"./shuffled_df/\", mode=\"overwrite\", compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mshuffled_df\u001b[00m\n",
      "├── \u001b[01;34mclass=setosa\u001b[00m\n",
      "│   ├── part-00000-aa6a5067-7f36-4e32-a005-4bdba0dce652.c000.snappy.parquet\n",
      "│   ├── part-00001-aa6a5067-7f36-4e32-a005-4bdba0dce652.c000.snappy.parquet\n",
      "│   ├── part-00002-aa6a5067-7f36-4e32-a005-4bdba0dce652.c000.snappy.parquet\n",
      "│   ├── part-00003-aa6a5067-7f36-4e32-a005-4bdba0dce652.c000.snappy.parquet\n",
      "│   └── part-00004-aa6a5067-7f36-4e32-a005-4bdba0dce652.c000.snappy.parquet\n",
      "├── \u001b[01;34mclass=versicolor\u001b[00m\n",
      "│   ├── part-00000-aa6a5067-7f36-4e32-a005-4bdba0dce652.c000.snappy.parquet\n",
      "│   ├── part-00001-aa6a5067-7f36-4e32-a005-4bdba0dce652.c000.snappy.parquet\n",
      "│   ├── part-00002-aa6a5067-7f36-4e32-a005-4bdba0dce652.c000.snappy.parquet\n",
      "│   ├── part-00003-aa6a5067-7f36-4e32-a005-4bdba0dce652.c000.snappy.parquet\n",
      "│   └── part-00004-aa6a5067-7f36-4e32-a005-4bdba0dce652.c000.snappy.parquet\n",
      "├── \u001b[01;34mclass=virginica\u001b[00m\n",
      "│   ├── part-00000-aa6a5067-7f36-4e32-a005-4bdba0dce652.c000.snappy.parquet\n",
      "│   ├── part-00001-aa6a5067-7f36-4e32-a005-4bdba0dce652.c000.snappy.parquet\n",
      "│   ├── part-00002-aa6a5067-7f36-4e32-a005-4bdba0dce652.c000.snappy.parquet\n",
      "│   ├── part-00003-aa6a5067-7f36-4e32-a005-4bdba0dce652.c000.snappy.parquet\n",
      "│   └── part-00004-aa6a5067-7f36-4e32-a005-4bdba0dce652.c000.snappy.parquet\n",
      "└── _SUCCESS\n",
      "\n",
      "3 directories, 16 files\n"
     ]
    }
   ],
   "source": [
    "!tree shuffled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Lowest number of files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_df = df.sort(\"class\")\n",
    "sorted_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sorted_df.write.partitionBy(\"class\").parquet(\"./sorted_df/\", mode=\"overwrite\", compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./sorted_df/\u001b[00m\n",
      "├── \u001b[01;34mclass=setosa\u001b[00m\n",
      "│   └── part-00000-8bcd65af-f831-4997-bffe-3786a4c03160.c000.snappy.parquet\n",
      "├── \u001b[01;34mclass=versicolor\u001b[00m\n",
      "│   └── part-00001-8bcd65af-f831-4997-bffe-3786a4c03160.c000.snappy.parquet\n",
      "├── \u001b[01;34mclass=virginica\u001b[00m\n",
      "│   └── part-00002-8bcd65af-f831-4997-bffe-3786a4c03160.c000.snappy.parquet\n",
      "└── _SUCCESS\n",
      "\n",
      "3 directories, 4 files\n"
     ]
    }
   ],
   "source": [
    "!tree ./sorted_df/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 50, 50, 0]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_df.rdd.mapPartitions(lambda it: [sum(1 for _ in it)]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- regrouping of data among partitions\n",
    "- expensive operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![mapreduce](https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2016/11/MapReduce-Way-MapReduce-Tutorial-Edureka-768x339.png)\n",
    "\n",
    "Image from https://www.edureka.co/blog/mapreduce-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- triggered by join and aggregation operations: joins, repartitioning, sorting, grouping, reducing\n",
    "- data can be sorted or hashed into partitions based on operation\n",
    "- use explain method to view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Sort [class#145 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(class#145 ASC NULLS FIRST, 5)\n",
      "   +- Scan ExistingRDD[sepal_length#141,sepal_width#142,petal_length#143,petal_width#144,class#145]\n"
     ]
    }
   ],
   "source": [
    "sorted_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- probably useful to experiment in a notebook to better understand the situation and possible fixes\n",
    "- start working locally, then with a small cluster, then bigger \n",
    "- cheaper and faster to catch problems at those earlier steps but not every problem can be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### EMR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- cluster summary\n",
    "- S3 logs for each node (bootstrap, steps, etc)\n",
    "- Spark history server\n",
    "- YARN application history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
